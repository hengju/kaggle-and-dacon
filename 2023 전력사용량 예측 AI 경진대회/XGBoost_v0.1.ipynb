{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2023 전력사용량 예측 AI 경진대회\n",
    "- 알고리즘 | 정형 | 시계열 | 에너지 | SMAPE\n",
    "- https://dacon.io/competitions/official/236125/overview/description\n",
    "\n",
    "- 참고 baseline 코드: https://dacon.io/competitions/official/236125/codeshare/8661?page=3&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 1860 개 의 이상치에 해당하는 행이 삭제됐습니다\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# 데이터 로드\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Seasonality 특성이 약한 건물 데이터만 남기고 나머지 제거 (EDA로 마킹한 자료 기준)\n",
    "check_data = pd.read_csv('building_small_seasonality.csv')\n",
    "# train_data = train_data[train_data['건물번호'].isin(check_data.building_num.tolist())]\n",
    "# test_data = test_data[test_data['건물번호'].isin(check_data.building_num.tolist())]\n",
    "# display(train_data['건물번호'].unique())\n",
    "# display(test_data['건물번호'].unique())\n",
    "\n",
    "# 이상치 제거 (EDA로 마킹한 자료 기준)\n",
    "outlier = pd.read_csv('outlier.csv')\n",
    "outlier['outlier'] = outlier['outlier'].apply(lambda x: x.split('|')[1])\n",
    "\n",
    "outlier_list = [i for i in outlier['outlier']]\n",
    "# print(outlier_list)\n",
    "\n",
    "string = ''\n",
    "for idx in range(len(outlier_list)):\n",
    "    string += outlier_list[idx]\n",
    "\n",
    "outlier_list = string.split(':')\n",
    "outlier_list.remove('')\n",
    "\n",
    "# 정의된 outlier에 해당하는 데이터 행 제거\n",
    "for i in outlier_list:\n",
    "    i = int(i)\n",
    "    train_data = train_data[~(train_data.index.isin(np.arange(i,i+20,1)))]\n",
    "\n",
    "print('총', len(outlier_list)*20, '개 의 이상치에 해당하는 행이 삭제됐습니다')\n",
    "# display(train_data)\n",
    "\n",
    "# 결측치 처리\n",
    "del train_data['일조(hr)']\n",
    "del train_data['일사(MJ/m2)']\n",
    "\n",
    "train_data['풍속(m/s)'] = train_data['풍속(m/s)'].interpolate(method='linear')\n",
    "train_data['습도(%)'] = train_data['습도(%)'].interpolate(method='linear')\n",
    "train_data['강수량(mm)'] = train_data['강수량(mm)'].fillna(0)\n",
    "test_data['강수량(mm)'] = test_data['강수량(mm)'].fillna(0)\n",
    "\n",
    "# 날짜 및 시간 특성 파생\n",
    "train_data['일시'] = pd.to_datetime(train_data['일시'], format='%Y%m%d %H')\n",
    "test_data['일시'] = pd.to_datetime(test_data['일시'], format='%Y%m%d %H')\n",
    "train_data['연'] = train_data['일시'].dt.year\n",
    "train_data['월'] = train_data['일시'].dt.month\n",
    "train_data['일'] = train_data['일시'].dt.day\n",
    "train_data['시간'] = train_data['일시'].dt.hour\n",
    "test_data['연'] = test_data['일시'].dt.year\n",
    "test_data['월'] = test_data['일시'].dt.month\n",
    "test_data['일'] = test_data['일시'].dt.day\n",
    "test_data['시간'] = test_data['일시'].dt.hour\n",
    "\n",
    "# 필요하지 않은 컬럼 제거\n",
    "train_data.drop(columns=['num_date_time'], inplace=True)\n",
    "train_data.set_index('일시',inplace=True)\n",
    "\n",
    "test_data.drop(columns=['num_date_time'], inplace=True)\n",
    "test_data.set_index('일시',inplace=True)\n",
    "\n",
    "### 컬럼 추가해보기\n",
    "\n",
    "# 불쾌지수\n",
    "train_data['THI'] = (9/5)*train_data['기온(C)'] - 0.55*(1-train_data['습도(%)'])*(9/5)*train_data['기온(C)']-26 + 32\n",
    "# 체감온도\n",
    "train_data['windchill'] = 13.12 + 0.6215*train_data['기온(C)'] - 11.37*train_data['풍속(m/s)']**0.16 + 0.3965*train_data['풍속(m/s)']**0.16*train_data['기온(C)']\n",
    "# 일평균기온\n",
    "# train_data['DayAvgTemp'] = train_data.groupby(['일'])['기온(C)'].transform('mean')\n",
    "# 일최대기온\n",
    "# train_data['DayMaxTemp'] = train_data.groupby(['일'])['기온(C)'].transform('max')\n",
    "# display(train_data)\n",
    "\n",
    "# 불쾌지수\n",
    "test_data['THI'] = (9/5)*test_data['기온(C)'] - 0.55*(1-test_data['습도(%)'])*(9/5)*test_data['기온(C)']-26 + 32\n",
    "# 체감온도\n",
    "test_data['windchill'] = 13.12 + 0.6215*test_data['기온(C)'] - 11.37*test_data['풍속(m/s)']**0.16 + 0.3965*test_data['풍속(m/s)']**0.16*test_data['기온(C)']\n",
    "# 일평균기온\n",
    "# test_data['DayAvgTemp'] = test_data.groupby(['일'])['기온(C)'].transform('mean')\n",
    "# 일최대기온\n",
    "# test_data['DayMaxTemp'] = test_data.groupby(['일'])['기온(C)'].transform('max')\n",
    "# display(test_data)\n",
    "\n",
    "\n",
    "# 특성과 라벨 분리\n",
    "X_train = train_data.drop(columns=['전력소비량(kWh)'])\n",
    "y_train = train_data['전력소비량(kWh)']\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(test_data), columns=test_data.columns, index=test_data.index)\n",
    "\n",
    "# 데이터 분할 (훈련 및 검증 세트)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Data Leakage 문제 예방하기 위해 데이터 비중이 아니라 일별 기준으로 train, valid set 분리\n",
    "# X_train_split = X_train_scaled[(X_train_scaled.index>='2022-06-01')&(X_train_scaled.index<'2022-08-15')]\n",
    "# X_val_split = X_train_scaled[(X_train_scaled.index>='2022-08-15')]\n",
    "# y_train_split = y_train[(y_train.index>='2022-06-01')&(y_train.index<'2022-08-15')]\n",
    "# y_val_split = y_train[(y_train.index>='2022-08-15')]\n",
    "\n",
    "X_train_split, X_val_split = X_train_split.to_numpy(), X_val_split.to_numpy()\n",
    "# print(X_train_split)\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'n_estimators': [5000],\n",
    "#     'subsample': [0.7, 0.9],\n",
    "#     'colsample_bytree': [0.7, 0.9],\n",
    "# }\n",
    "\n",
    "# SMAPE(성능지표) 정의\n",
    "# def SMAPE(true, pred):\n",
    "#     return np.mean((np.abs(true-pred))/(np.abs(true) + np.abs(pred))) * 100\n",
    "\n",
    "# smape = make_scorer(SMAPE, greater_is_better=False)\n",
    "\n",
    "# XGBoost 모델 생성\n",
    "# xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# xgb = XGBRegressor(objective='reg:squarederror',\n",
    "#                    max_depth=7,\n",
    "#                    subsample=0.9,\n",
    "#                    colsample_bytree=0.9,\n",
    "#                    random_state=42)\n",
    "\n",
    "# random search로 best parameter 찾기\n",
    "# grid_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_grid,\n",
    "#                            cv=3, n_jobs=3, verbose=2, scoring=smape)\n",
    "\n",
    "# grid search로 best parameter 찾기\n",
    "# param_grid = {\n",
    "#     'n_estimators':[2000,3000,5000],\n",
    "#     'learning_rate': [0.01,0.05,0.1],\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=2, scoring='neg_root_mean_squared_error',\n",
    "#                            n_jobs=3, verbose=2)\n",
    "\n",
    "# grid_search.fit(X_train_split, y_train_split,\n",
    "#                 eval_set=[(X_val_split, y_val_split)],\n",
    "#                 early_stopping_rounds=10,\n",
    "#                 verbose=True\n",
    "#                 )\n",
    "\n",
    "# 최종 모델으로 학습\n",
    "# print('Best Parameter:', grid_search.best_params_)\n",
    "# model = grid_search.best_estimator_\n",
    "\n",
    "model = XGBRegressor(objective='reg:squarederror',\n",
    "                    n_estimators=2000,\n",
    "                    max_depth=7,\n",
    "                    learning_rate=0.01,\n",
    "                    subsample=0.9,\n",
    "                    colsample_bytree=0.9,\n",
    "                    random_state=42,\n",
    "                    )\n",
    "\n",
    "# 교차검증 시행\n",
    "# scores = cross_validate(model, X_train_split, y_train_split, cv=2,\n",
    "#                         scoring='neg_root_mean_squared_error',\n",
    "#                         n_jobs=3)\n",
    "\n",
    "# print(scores)\n",
    "\n",
    "# 모델 학습 및 파일로 저장\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "filename = 'test-xgboost-model.sav'\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "# 추론 결과 저장\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['answer'] = y_pred\n",
    "\n",
    "submission.to_csv('my_submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
